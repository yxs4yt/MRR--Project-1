---
title: "DS 4002 Project 1"
author: "Morgan Simmons"
date: "2026-02-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#---------------------------
# 1) Read the raw dataset
#---------------------------

# read.csv() loads a CSV file from disk into an R data frame.
# Here, we’re reading the job postings dataset into an object named `jobs`.
# Purpose: get the raw data into R so we can clean and engineer features from it.
jobs <- read.csv("data_science_job_posts_2025.csv")


#---------------------------
# 2) Define your AI-skill "dictionary"
#---------------------------

# This is a character vector (a list of strings) containing the skill keywords
# you want to treat as “AI-related”.
# Purpose: later we’ll check each job’s listed skills against this vector
# and count how many matches occur.
ai_skills <- c(
  "machine learning",
  "deep learning",
  "neural network",
  "scikit-learn",
  "pytorch",
  "tensorflow",
  "numpy"
)

```

```{r}
#---------------------------
# 3) Load packages you need
#---------------------------

# tidyverse is a bundle of packages (dplyr, tidyr, stringr, ggplot2, etc.)
# that provide a consistent syntax for data manipulation.
# Purpose: we use dplyr verbs (%>%, select, filter, mutate, across),
# tidyr tools (separate), and stringr tools (str_remove_all, str_trim).
library(tidyverse)


#---------------------------
# 4) Clean + reshape the skills column
#---------------------------

# Start with the raw jobs data frame and pipe (%>%) it through a sequence
# of transformations. The pipe passes the current data frame to the next step.
jobs_clean <- jobs %>%
  
  # select() keeps only the columns you want to work with.
  # Purpose: reduce clutter and make the next steps faster/clearer.
  select(job_title, location, headquarter, industry, skills) %>%
  
  # filter() keeps only rows that meet a condition.
  # Here you keep rows where the `skills` column is not literally "[]".
  # Purpose: remove postings that have an empty skills list so ratios/counts
  # aren’t computed on meaningless skill sets.
  filter(skills != "[]") %>%
  
  # mutate() creates or modifies columns.
  # Here we overwrite `skills` by stripping out characters used to represent
  # lists/quotes, e.g. [ ] " '.
  #
  # str_remove_all() removes all matches of a regex from a string.
  # The pattern "\\[|\\]|\"|'" means:
  #   - \\[  : a literal '['
  #   - |    : OR
  #   - \\]  : a literal ']'
  #   - |\"  : a literal double quote
  #   - |'   : a literal single quote
  #
  # Purpose: convert something like ["python","numpy","sql"] into
  # something like python,numpy,sql so we can split it reliably.
  mutate(
    skills = str_remove_all(skills, "\\[|\\]|\"|'")
  ) %>%
  
  # separate() splits one column into multiple columns using a delimiter.
  # - into = c("skill_1","skill_2","skill_3"): the new column names
  # - sep = ",": split on commas
  # - extra = "drop": if there are MORE than 3 skills, discard the extras
  # - fill = "right": if there are FEWER than 3 skills, fill missing ones with NA
  #
  # Purpose: create fixed columns (skill_1..skill_3) so we can count skills
  # and compare them to ai_skills. This is a “wide” representation.
  separate(
    skills,
    into = c("skill_1", "skill_2", "skill_3"),
    sep = ",",
    extra = "drop",
    fill = "right"
  ) %>%
  
  # mutate(across(...)) applies a function to multiple columns at once.
  # - starts_with("skill_") picks columns skill_1, skill_2, skill_3
  # - str_trim() removes leading/trailing whitespace
  #
  # Purpose: after separating, skills often look like " numpy" or "sql "
  # which would fail exact matching unless we trim whitespace.
  mutate(across(starts_with("skill_"), str_trim))


#---------------------------
# 5) Feature engineering: totals + AI counts + ratios
#---------------------------

jobs_clean <- jobs_clean %>%
  
  # mutate() again to add computed columns:
  mutate(
    # total_skills:
    # We want to count how many skill columns are actually present (not NA) per row.
    #
    # across(starts_with("skill_")) returns the skill columns.
    # is.na(...) checks which entries are NA.
    # !is.na(...) converts that into TRUE for "has a value", FALSE for NA.
    # rowSums(...) adds up TRUE/FALSE across columns for each row
    # (TRUE counts as 1, FALSE counts as 0).
    #
    # Purpose: know the denominator for the AI ratio.
    total_skills = rowSums(!is.na(across(starts_with("skill_")))),
    
    # ai_skill_count:
    # We check each skill column to see if it appears in the ai_skills vector.
    #
    # across(..., ~ .x %in% ai_skills) applies the expression to each skill column:
    #   - .x is the column vector
    #   - %in% returns TRUE if the skill is exactly one of ai_skills, else FALSE
    #
    # rowSums(..., na.rm = TRUE) then counts how many TRUEs are in that row.
    # na.rm = TRUE ensures NAs don’t break the sum.
    #
    # Purpose: count how many of the listed skills are AI-related.
    ai_skill_count = rowSums(
      across(starts_with("skill_"), ~ .x %in% ai_skills),
      na.rm = TRUE
    ),
    
    # ai_skill_ratio:
    # A simple proportion: (# AI skills) / (total # skills).
    #
    # Purpose: normalize AI emphasis so postings with 1 skill vs 3 skills
    # are comparable.
    ai_skill_ratio = ai_skill_count / total_skills
  )

```

```{r}
library(dplyr)

#--------------------------------------------
# Create Summary Statistics Table
#--------------------------------------------

summary_table <- jobs_clean %>%
  summarise(
    
    # Count total number of job postings (rows in dataset)
    Total_Postings = n(),
    
    # Average number of skills listed per posting
    Avg_Total_Skills = mean(total_skills, na.rm = TRUE),
    
    # Average number of ML/AI skills per posting
    Avg_AI_Skills = mean(ai_skill_count, na.rm = TRUE),
    
    # Mean proportion of ML/AI skills per posting
    Mean_AI_Skill_Ratio = mean(ai_skill_ratio, na.rm = TRUE),
    
    # Standard deviation of ML/AI skill ratio
    SD_AI_Skill_Ratio = sd(ai_skill_ratio, na.rm = TRUE),
    
    # Minimum ML/AI skill ratio
    Min_AI_Skill_Ratio = min(ai_skill_ratio, na.rm = TRUE),
    
    # Maximum ML/AI skill ratio
    Max_AI_Skill_Ratio = max(ai_skill_ratio, na.rm = TRUE)
  )

summary_table

library(tidyr)

summary_table_clean <- summary_table %>%
  pivot_longer(
    cols = everything(),
    names_to = "Statistic",
    values_to = "Value"
  )

summary_table_clean

summary_table_clean <- summary_table_clean %>%
  mutate(Value = round(Value, 3))

library(knitr)
kable(summary_table_clean, caption = "Table 1: Summary Statistics of Job Posting Dataset")

```

```{r}
#------------------------------------------------------------
# TABLE 2: Ranked Frequency of Top Skills
#------------------------------------------------------------

# Step 1: Convert skill_1, skill_2, skill_3 into one long column
# Purpose: We need one row per skill occurrence in order to count frequencies

skills_long <- jobs_clean %>%
  select(starts_with("skill_")) %>%        # Select only skill columns
  pivot_longer(
    cols = everything(),                   # Pivot all selected columns
    names_to = "skill_position",           # Temporary column indicating skill_1, skill_2, etc.
    values_to = "skill"                    # New column containing the actual skill name
  ) %>%
  filter(!is.na(skill))                    # Remove missing skills


# Step 2: Count total job postings
# Purpose: Needed to compute percentage of postings requesting each skill

total_postings <- nrow(jobs_clean)


# Step 3: Calculate frequency, percentage, and rank
# Purpose: Create final ranked skill demand table

table2 <- skills_long %>%
  group_by(skill) %>%                      # Group by unique skill name
  summarise(
    Frequency = n()                        # Count how many times each skill appears
  ) %>%
  mutate(
    Percentage_of_Postings = 
      round((Frequency / total_postings) * 100, 2)  # Convert to percentage
  ) %>%
  arrange(desc(Frequency)) %>%             # Sort from most to least frequent
  mutate(
    Rank = row_number()                    # Assign rank based on frequency order
  ) %>%
  select(Rank, skill, Frequency, Percentage_of_Postings)  # Reorder columns


# Optional: Show only top 10 skills for appendix table
top10_table2 <- table2 %>%
  slice(1:10)

top10_table2
```

```{r}
#------------------------------------------------------------
# FIGURE 1: Distribution of AI_Skill_Ratio
#------------------------------------------------------------

# Create histogram of AI skill ratios
# Purpose: Visualize how ML/AI skills are distributed across job postings

ggplot(jobs_clean, aes(x = ai_skill_ratio)) +
  
  # Histogram layer
  geom_histogram(
    binwidth = 0.1,           # Each bar covers 0.1 range (0–0.1, 0.1–0.2, etc.)
    color = "black",          # Black borders around bars
    fill = "steelblue"        # Fill color of bars
  ) +
  
  # Add labels and title
  labs(
    title = "Distribution of ML/AI Skill Ratio in Data Science Job Postings",
    x = "AI Skill Ratio (Proportion of Skills that are ML/AI-Related)",
    y = "Number of Job Postings"
  ) +
  
  # Clean theme for professional presentation
  theme_minimal()
#------------------------------------------------------------
# Save Figure 1 as a PNG file
#------------------------------------------------------------

ggsave(
  filename = "Figure1_AI_Skill_Ratio_Distribution.png",  # Name of file
  width = 8,                                              # Width in inches
  height = 6,                                             # Height in inches
  dpi = 300                                               # High resolution for print
)

```

```{r}
#------------------------------------------------------------
# TABLE 3: Distribution of ML/AI Skill Counts per Posting
#------------------------------------------------------------

# Total number of job postings (needed for percentages)
total_postings <- nrow(jobs_clean)

# Create distribution table
table3 <- jobs_clean %>%
  
  # Count how many postings fall into each AI skill count category
  group_by(ai_skill_count) %>%
  summarise(
    Number_of_Postings = n()
  ) %>%
  
  # Ensure all categories 0,1,2,3 appear (even if count is 0)
  complete(ai_skill_count = 0:3,
           fill = list(Number_of_Postings = 0)) %>%
  
  # Calculate percentage of total postings
  mutate(
    Percentage_of_Postings = 
      round((Number_of_Postings / total_postings) * 100, 2)
  ) %>%
  
  # Rename for clean presentation
  rename(
    AI_Skill_Count = ai_skill_count
  ) %>%
  
  arrange(AI_Skill_Count)

table3

```

```{r}
write.csv(summary_table_clean, "Table1_Summary_Statistics.csv", row.names = FALSE)

write.csv(top10_table2, "Table2_Top10_Skills.csv", row.names = FALSE)

write.csv(table3, "Table3_AI_Skill_Distribution.csv", row.names = FALSE)
```

